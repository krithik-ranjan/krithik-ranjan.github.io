<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link href="https://fonts.googleapis.com/css2?family=Karla:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;1,200;1,300;1,400;1,500;1,600;1,700;1,800&display=swap" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <title>ECE 3400 Spring 2021 Wiki by kr397</title>
  </head>

  <body>
    <header>
      <div class="inner">
        <h1>ECE 3400 Wiki</h1>
        <h2>Spring 2021</h2>
        <h2><b>Krithik Ranjan</b> kr397</h2>
        <!--
        <a href="https://github.coecis.cornell.edu/kr397/ece3400-sp2021" class="button"><small>View project on</small> GitHub</a>
        -->
      </div>
    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">
          <h1>
<a id="lab-4-ultrasonic-and-all" class="anchor" href="#lab-4-ultrasonic-and-all" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><strong>Lab 4</strong> Ultrasonic Sensors and All</h1>

<h2>
<a id="objective" class="anchor" href="#objective" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Objective</h2>
<p>
    This was the final lab for the ECE 3400 course which culminated by integrating various elements learned throughout the course. The goal of the lab was to build two types of autonomous robots that responds to its environment differently. Both of these systems were designed to perform Demo 1 and Demo 2, as they were outlined in the lab handout. In Demo 1, the robot responded to objects placed in front of it by turning on its onboard LED while turning around in-place, and in Demo 2, the robot moved around a space directed by a flashlight and detecting various obstacles. While some of the components from previous labs were used for the robot, like the microphone with amplifier and filter, and the light sensors, in this lab we were also introduced to the ultrasonic sensor (HC-SR04) to determine the distance of objects placed in front of the robot. This lab served as a great conclusion to the course as we used all the several of the concepts from the course to actually build an "Intelligent Physical System".
</p>

<h2>
    <a id="materials-used" class="anchor" href="#materials-used" aria-hidden="true">
        <span aria-hidden="true" class="octicon octicon-link">
        </span>
    </a>
    Materials Used
</h2>
<ul>
    <li>Arduino Nano Every</li>
    <li>Electret microphone</li>
    <li>LM358P Operational Amplifier</li>
    <li>L293D Motor Driver IC</li>
    <li>Photoresistors</li>
    <li>HC-SR04 Ultrasonic Distance Sensor</li>
    <li>DC Geared Motors</li>
    <li>Resistors</li>
    <li>Capacitors</li>
    <li>Jumpers</li>
    <li>Breadboard</li>
</ul>    

<h2>
<a id="using-the-ultrasonic-sensor" class="anchor" href="#using-the-ultrasonic-sensor" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Using the Ultrasonic Sensor</h2>
<p>
    In the first part of the lab, we familiarized ourselves with the HC-SR04 ultrasonic sensor. Since the robot is supposed to detect the objects in front of it, we first attached the sensor to the robot using velcro as described in the lab handout. For this, we had to relocate the previously placed 9V battery to form a surface for the sensor, and then mounted the sensor in front as shown. The HC-SR04 is a 4-pin sensor, which includes 5V power, ground, and two digital pins, <code>ECHO</code> and <code>TRIG</code>. These pins were connected to the Arduino Nano Every accordingly. Since the rest of the robot was carried forward from Lab 2, installing the ultrasonic sensor completed the hardware setup for this lab.
</p>
<figure>
    <img src="images/lab4/robot_sensor.jpg">
    <img src="images/lab4/robot_full.jpg">
    <figcaption><center><i>Figure: Placement of the Ultrasonic sensor on the robot</i></center></figcaption>
</figure>
<p>
    Once the HC-SR04 was properly integrated with the robot, we had to write Arduino sketches to test and characterize the sensor. Coding of the ultrasonic sensor is unlike other simple sensors that use <code>analogRead</code> (for example, the light sensor). The distance of an object in front of the sensor is determined by echolocation (<i>the location of objects by reflected sound</i> as defined by the Oxford Dictionary). In our sensor, the <code>TRIG</code> pin is used to emit an ultrasonic sound wave for a certain duration of time, the reflection of which from the object is detected by the <code>ECHO</code> pin and the time interval between the emission and detection is used to determine the exact distance of the object. It is half the time taken by the sound wave to reflect (to and from the object) multiplied the speed of sound in air (343 m/s). The sample code for one distance measurement using the HC-SR04 is given below. An important thing to note about the ultrasonic sensor is that it can't be successively read. Its datasheet suggests an interval of at least 60 ms (ideally 100 ms) between consecutive measurements.
</p>
<pre><code>// Low signal before sending the pulse 
digitalWrite(TRIG_PIN, LOW);
delayMicroseconds(2);

// Pulse for 10us
digitalWrite(TRIG_PIN, HIGH);
delayMicroseconds(10);
digitalWrite(TRIG_PIN, LOW);

// Measure the reflection on ECHO using pulseIn function 
sound_duration = pulseIn( ECHO_PIN, HIGH );

// Calculate the distance in cm using the following formula
distance = ( sound_duration * 0.0343 ) / 2;</code></pre>
<p>
    After testing the readings from the ultrasonic sensor, we were also required to characterize its accuracy by obtaining its measurements against a list of reference values. This was done by placing an object a certain distance away from the sensor and noting down the value measured by the sensor. The ultrasonic measurements along with error has been plotted against the actual object distance below. As we can see from the plot, the sensor seems to be fairly accurate against a range of different sizes. The variations at certain distances might also be due to imprecise manual measurement betwween the sensor and the object (as it was done simply using a ruler by estimating the position of the sensor on the surface).
</p>
<figure>
    <center><i>
        <img src="images/lab4/hcsr04-distance.png">
        <figcaption>Figure: HC-SR04 Distance measurements and error vs. actual distance</figcaption>
    </i></center>
</figure>

<h2>
<a id="light-sensors-and-microphone" class="anchor" href="#light-sensors-and-microphone" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Light Sensors, the Microphone and Motion</h2>
<p>
    As mentioned earlier, the robot in this lab featured elements from the previous labs, namely, the microphone circuit and the light sensors. The microphone circuit was used to detect a sound being played of a certain frequency and respond accordingly, while the light sensors were used to navigate in the direction of a light source near the robot. The setup of both of these were very similar to that in the previous labs. 
</p>
<p>
    Two photo-resistor light sensors had been installed on either side of the robot in Lab 2 to detect changes in ambient light and also the side of the robot which had more light. As described in the <a href="lab2.html">Lab 2</a> webpage, the photo-resistor was connected in a voltage divider circuit and the voltage level was measured from the ADC using the <code>analogRead</code> function. The direction of light source was determined by normalizing the sensor readings.  
</p>
<p>
    The microphone, with its added circuits for amplification and filtering, was assessed in-depth in <a href="lab3.html">Lab 3</a>. For this lab, we retained the final circuit consisting of the microphone, an amplifier, and a bandpass filter. The sound output was then measured with the ADC on the Arduino, and the <code>FFT</code> library was used to calculate the Fourier transform of the sound signal and determine if frequency was detected. The FFT library functions calculate a 256-point FFT from 256 values measured at intervals of 0.41667 ms (corresponding to sampling rate of 2400 Hz) with the help of the ADC in free-running mode and a periodic timer interrupt.
</p>
<p>
    The mobile robot in this lab has been setup exactly like in Lab 2, with the help of the given robot kit including the chassis, motors, wheels, etc. In order to control the robot, the robot circuit consists of the L293D motor driver IC which is connected to digital pins on the Arduino. These pins are manipulated to turn the motors in either direction at a certain speed using PWM. 
</p>

<h2>
<a id="demo-1" class="anchor" href="#demo-1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Demo 1</h2>
<p>
    In the first demonstration for this lab, the robot was supposed to wait for a sound of a certain frequency (550 Hz) and then move around in place to detect two objects placed nearby. This sound was to be detected from between a melody of different sounds provided along with the lab. We first also had to characterize the provided sound file with FFT to determine all the frequencies present in it. I did this with the help of MATLAB and obtained the following frequency spectrum. As can be seen from the peaks in the plot, the melody consists of 550 Hz, 700 Hz, and 900 Hz sounds, out of which, our robot is only supposed to respond to the 550 Hz. 
</p>
<figure>
    <img src="images/lab4/demo1_spectrum.png">
    <figure><center><i>Figure: Spectrum of the sample sound file provided for Demo 1</i></center></figure>
</figure>
<p>
    After characterizing the given melody, we had to program the robot to perform the entire demo 1 sequence, as it had been described in the lab handout. The demo comprised of two main steps, summarized as follows.
    <ul>
        <li><b>Step 1: </b>The robot starts off motionless and with the onboard LED off. It waits in this state until the sound of 550 Hz frequency is detected, and then moves to the next step.</li>
        <li><b>Step 2: </b>Once the sound is detected, the robot starts rotating in either direction over the next 4-5 seconds. While rotating, the robot is supposed to detect when it faces either obstacle 1 or obstacle 2, and turn its on-board LED on. The LED should be off when there is no obstacle in front. After completing almost 1 full rotation, the robot should return to idle stopped state with LED off.</li>
    </ul>
    The setup of the two obstacles around the robot is required to be as shown in the figure below, with obstacle 1 about 20-30 cm away and obstacle 2 60-70 cm away. 
</p>
<figure>
    <img src="images/lab4/demo1_arena.png">
    <figure><center><i>Figure: Setup for Demo 1 (from the Lab handout)</i></center></figure>
</figure>
<p>
    In order to implement the demo 1 on the robot with the Arduino Nano Every, I setup the sketch to execute three different states.
    <ul>
        <li><code>STATE_FFT</code>: The initial state of the robot where it is waiting to detect the 550 Hz frequency sound. In this state, the Arduino repetitively listens on the microphone for a period of time, computes the FFT of that sound interval using the FFT library, and determines if a peak exists at 550 Hz at the spectrum of the FFT. As described earlier, the Arduino measures 256 values from the microphone and computes the 256-point FFT, and if the 550 Hz is not detected, gets another 256 readings. In order to detect the desired frequency, the Arduino first checks if the maximum value in the spectrum is nearby the desired frequency, and also if that that value is significantly higher than rest of the frequencies. For the first part, it simply iterates over the FFT spectrum array to find the maximum and then determines the corresponding frequency. The first few bins are ignored in order to not account for the DC offset. To do the latter, the Arduino compares the max value to the 4 times the average of all the bins. If the max frequency bin is within 4 bins of the desired frequency (2 on either side), and if the bin is an actual peak, it means that the frequency has been detected. The factor of 4 and the range of 4 have been determined to be appropriate through multiple tries, given the fact that no other frequencies are present within 150 Hz of the desired frequency of 550 Hz. These stpes to determine if the sound has been detected have been summarized in the pseudocode below.
        <pre><code> desired_freq = 550
// FFT bin of the desired frequency
bin = desired_freq * 128 / 1200
// Find the highest magnitude bin in the FFT, ignoring first 8 bins
max_bin = argmax(fft_out[8:128])
// Find the average magnitude 
fft_avg = avg(fft_out[8:128])
found = ( fft_out[max_bin] > 4 * fft_avg ) or (abs(bin - max_bin) <= 2)</code></pre>
        Once the frequency is detected, the robot moves to the next state. 
        </li>
        <li><code>STATE_MOVE</code>: In this state after the sound has been detected, the robot rotates in its place for 5 seconds. It constantly performs distance measurements on the ultrasonic sensor (at intervals of 100 ms) to check if either of the obstacles are present in front of it. It does this by comparing the measured distance with a preset threshold (slightly higher than the actual distance of the obstacles from the robot). If either of the obstacles are present within the threshold, the onboard LED is turned on, until the robot isn't facing the obstacle anymore. Once 5 seconds are over and an entire rotation has completed (not exactly), the robot moves to the next state. 
        </li>
        <li><code>STATE_DONE</code>: This is the final state of the robot, where it stays idle with its LED off, indefinitely, like the <code>STATE_FFT</code>. However, there is no FFT computation involved in this state.</li>
    </ul>
</p>
<figure>
    <img src="images/lab4/demo1_video.gif">
    <center><figcaption><i>Video: Robot performing Demo 1</i></figcaption></center>
</figure>
    
<h2>
<a id="demo-2" class="anchor" href="#demo-2" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Demo 2</h2>
<p>
    The second demonstration for this lab was quite different from the first one, using the light sensors and the ultrasonic sensor instead of the microphone circuit. The area was set up with three different obstacles at fixed locations, and the robot was to perform a sequence of steps involving moving in the direction of a light source and stopping in front of obstacles. The setup of the space and the movement of the robot has been illustrated in the figure below. 
</p>
<figure>
    <img src="images/lab4/demo2_arena.png">
    <figcaption><center><i>Figure: Setup for Demo 2 (from the Lab handout)</i></center></figcaption>
</figure>
<P>
    This demo consisted of several steps described in the lab handout, which have been summarized as follows. 
    <ul>
        <li><b>Step 1: </b>The robot starts off motionless at its initial location and the onboard LED stays off. The robot stays in this state until a bright light is shown on it, after which, it starts moving in the direction of the light.</li>
        <li><b>Step 2: </b>Once the robot is moving, we need to lure it with the help of the flashlight towards obstacle 1 in the path shown. The onboard LED stays off and only turns on when the robot detects the obstacle in its front (30 cm away). The robot keeps moving until it reaches less than 5 cm away from the obstacle.</li>
        <li><b>Step 3: </b>In front of the obstacle, the robot remains stopped with its LED on. It does not respond to light shown on it from the front, or the left, but only starts moving when the light is on the right.</li>
        <li><b>Step 4: </b>As the robot starts moving, it will turn off its LED and be directed by the light source towards obstacle 2, through the path shown in the figure. The LED will only turn on when the robot can detect obstacle 2 in front of it (less than 30 cm).</li>
        <li><b>Step 5: </b>The robot stops when it reaches less than 5 cm in front of obstacle 2, after which it turns off its LED.</li>
        <li><b>Step 6: </b>Once stopped, the robot will then be directed to turn towards the back of the obstacle 1 with its LED on, and move towards it in the direction of the light. When the robot is 5 cm away from the obstacle, it will stop.</li>
        <li><b>Step 7: </b>Now, the robot will stay motionless until it is lured by the flashlight to rotate in place and move towards obstacle 3, during which, its LED will stay off.</li>
        <li><b>Step 8: </b>Finally, the robot will move until it reaches 5 cm away from obstacle 3, and then it will stop and turn the LED on. </li>
    </ul>
</P>
<p>
    To implement the robot such that it successfully performs all these steps in the demo, I used various robot states in the Arduino sketch. 
    <ul>
        <li><code>STATE_START</code>: The initial state of the robot where it stays motionless until a light source is shown. The Arduino continuously measures the light readings from both photo-sensors and compares them against the older running average of both sensor values. If the new readings on either sensor are more than a threshold than the average value, it means that the light source is detected, and the robot will move to the next state. If the light source is not detected, the new readings are added to update the running average of both the sensors. This is very similar to the transition of the robot from <code>STATE_IDLE</code> to <code>STATE_FOUND</code> with the running average algorithm in Lab 2.</li>
        <li><code>STATE_START_OB1</code>: This is the movement of the robot from the initial state to obstacle 1, while being directed by the light source. The robot compares the light sensor readings through normalized measurements to determine the side of the robot that has the light, and turn towards it. If the measurements from both the sensor are similar, the robot simply move straight. This state remains until the robot reaches in front of obstacle 1 (5 cm away), after which it stops and moves to the next state. During this movement, the LED starts as off, and turns on when the robot detects obstacle 1 30 cm away, and stays on after it stops.</li>
        <li><code>STATE_OB1_A</code>: Once stopped in front of obstacle 1, the robot does not respond to light shown from the front or the left. Only when the light is on the right, it turns towards the light until it faces it, and then moves to the next state.The LED is off during this movement.</li>
        <li><code>STATE_OB1_OB2</code>: This state represents the movement of the robot from obstacle 1 to obstacle 2. This is similar to <code>STATE_START_OB1</code> where it moves in the direction of the light until reaches 5 cm away from obstacle 2. The LED turns on when the obstacle detected 30 cm away, but turns off when the robot stops at 5 cm.</li>
        <li><code>STATE_OB2</code>: Similar to <code>STATE_OB1_A</code> with the robot stopped in front of obstacle 2 and doesn't respond to light from the front or the right. Only turns when the light shown from the left after turning the LED on.</li>
        <li><code>STATE_OB2_OB1</code>: The movement of the robot from obstacle 2 to the back of the obstacle 1 while following the light source. Since obstacle is already less than 30 cm away, the LED stays on. The state changes when the obstacle closer than 5 cm.</li>
        <li><code>STATE_OB1_B</code>: This state is just like <code>STATE_OB1_A</code> but instead from the back of obstacle 1. Now, the robot only responds to light shown from the left and turns with its LED off.</li>
        <li><code>STATE_OB1_OB3</code>: The robot now follows the light to be directed to obstacle 3. Once 5 cm away, stop and turns on the LED.</li>
        <li><code>STATE_OB3</code>: Final state of the robot where it stays stopped with its LED on.</li>
    </ul> 
</p>
<figure>
    <img src="images/lab4/demo2_video.gif">
    <figcaption><center><i>Video: Robot performing Demo 2</i></center></figcaption>
</figure>

<h2>
<a id="requirements-and-constraints" class="anchor" href="#requirements-and-constraints" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Requirements and Constraints</h2>
<p>
    Over the two demos in this lab, we were provided with some constraints on the implementation of the robot. These were incorporated while coding all the states that have been described above. These constraints are as follows:
    <ul>
        <li>The robot must move slowly.</li>
        <li>Cannot use any library (other than the FFT library in Demo 1).</li>
        <li>Cannot use functions like <code>attachInterrupt</code>.</li>
        <li>Cannot use the <code>delay</code> or any other blocking function or code.</li>
        <li>Use the amplified microphone circuit for Demo 1, and ideally the bandpass filter.</li>
        <li>During the recording of the demos, the TX and RX LEDs must remain off (no serial communications).</li>
        <li>Record a complete video for both the demos without any kind of cheating.</li>
    </ul> 
</p> 

<h2>
<a id="testing-and-problems" class="anchor" href="#testing-and-problems" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Testing and Problems</h2>
<p>
    This lab had been broadly categorized into two sections through the two independent demos. The theme of the lab was to create a final robot comprising of various elements that had been explored in previous and this lab. With such an approach, all of the parts had been tested separately before being integrated into the robot; for example, the light sensors in Lab 2, the microphone circuit in Lab 3. In the beginning of this lab, we independently tested and characterized the ultrasonic sensor before using it in the robot. Once the robot had been consolidated and programmed to perform the demos, the primary form of testing was observation. Before actually putting the robot down to move around, I had disabled the motors and kept it connected to the serial monitor to look at the log of the different states of the robot and also their transitions. Finally, it took several tries and practices until the robot moves perfectly for both the demos. 
</p>
<p>
    Since the ultrasonic sensor was the only new component in this lab, there were few problems with it through the lab. I had initially not explicitly maintained the 100 ms time interval between two successive reads from the sensor, which caused it to blurt out some random values every once in a while (for e.g. greater than 1000 cm when an object was right in front of the sensor). Even after introducing the time delay between two reads, the sensor occasionally gave a value of 0.0 cm which caused the robot states to change preemptively. This was fixed by ensuring that the sensor never returns a very small (near zero) value. Finally, since the sensor measures distance using the reflection of sound waves, there was a physical consideration. The object placed in front of the robot could not be made of a soft material, like cloth, which would absorb most of the sound waves hitting on it. I realized this after a few initial tests with a soft object that gave incorrect results.  
</p>

        </section>

        <aside id="sidebar">
          <h3>Back to <a href="index.html">Home</a></h3>
          <h2><a href="#objective">Objective</a></h2>
          <h2><a href="#materials-used">Materials Used</a></h2>
          <h2><a href="#using-the-ultrasonic-sensor">Using the Ultrasonic Sensor</a></h2>
          <h2><a href="#light-sensors-and-microphone">Light Sensors, the Microphone and Motion</a></h2>
          <h2><a href="#demo-1">Demo 1</a></h2>
          <h2><a href="#demo-2">Demo 2</a></h2>
          <h2><a href="#requirements-and-constraints">Requirements and Constraints</a></h2>
          <h2><a href="#testing-and-problems">Testing and Problems</a></h2>
          <br>
          <p class="repo-owner"><a href="https://github.coecis.cornell.edu/kr397/ece3400-sp2021"></a>Maintained by <a href="https://github.coecis.cornell.edu/kr397">kr397</a>.</p>

          <p>This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the Architect theme by <a href="https://twitter.com/jasonlong">Jason Long</a>.</p>
        </aside>
      </div>
    </div>

  
  </body>
</html>
